{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textClassificationUsingCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNdwjGsvSXzW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pydot"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DndM9KdrUVsq"
      },
      "source": [
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def remove_url(text): \n",
        "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    return url_pattern.sub(r'', text)\n",
        " # converting return value from list to string\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text ): \n",
        "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
        "    delete_dict[' '] = ' ' \n",
        "    table = str.maketrans(delete_dict)\n",
        "    text1 = text.translate(table)\n",
        "    #print('cleaned:'+text1)\n",
        "    textArr= text1.split()\n",
        "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>2))]) \n",
        "    \n",
        "    return text2.lower()"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEHzFUeGUa28"
      },
      "source": [
        "train_data= pd.read_csv(\"train.csv\")\n",
        "train_data.dropna(axis = 0, how ='any',inplace=True) \n",
        "train_data['Num_words_text'] = train_data['message'].apply(lambda x:len(str(x).split())) \n",
        "mask = train_data['Num_words_text'] >2\n",
        "train_data = train_data[mask]\n",
        "max_train_sentence_length  = train_data['Num_words_text'].max()\n",
        "\n",
        "\n",
        "train_data['message'] = train_data['message'].apply(remove_emoji)\n",
        "train_data['message'] = train_data['message'].apply(remove_url)\n",
        "train_data['message'] = train_data['message'].apply(clean_text)"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beTwWxIfXVOh"
      },
      "source": [
        "num_words = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words,oov_token=\"unk\")\n",
        "tokenizer.fit_on_texts(train_data['message'].tolist())\n"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA_P3NoiXhTO",
        "outputId": "dd0bef27-e630-45b3-e562-71ffdd136774"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(train_data['message'].tolist(),\\\n",
        "                                                      train_data['result'].tolist(),\\\n",
        "                                                      test_size=0.1,\\\n",
        "                                                      stratify = train_data['result'].tolist(),\\\n",
        "                                                      random_state=0)\n",
        "\n",
        "x_train = np.array( tokenizer.texts_to_sequences(X_train) )\n",
        "x_valid = np.array( tokenizer.texts_to_sequences(X_valid) )\n",
        "\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=40)\n",
        "x_valid = pad_sequences(x_valid, padding='post', maxlen=40)\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "train_labels = le.fit_transform(y_train)\n",
        "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels))\n",
        "valid_labels = le.transform(y_valid)\n",
        "valid_labels = np.asarray( tf.keras.utils.to_categorical(valid_labels))\n",
        "\n",
        "list(le.classes_)\n",
        "\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,valid_labels))"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCIJo-LsYdQE"
      },
      "source": [
        "train_labels = le.fit_transform(y_train)\n",
        "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels))"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYhlnpxnY8A8",
        "outputId": "a74f9908-9979-4f12-e7f1-1c1dcda6eeec"
      },
      "source": [
        "max_features =20000\n",
        "embedding_dim =64\n",
        "sequence_length = 40\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, input_length=sequence_length,\\\n",
        "                                    embeddings_regularizer = regularizers.l2(0.0005)))                                    \n",
        "\n",
        "model.add(tf.keras.layers.Conv1D(128,3, activation='relu',\\\n",
        "                                 kernel_regularizer = regularizers.l2(0.0005),\\\n",
        "                                 bias_regularizer = regularizers.l2(0.0005)))                               \n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(3, activation='sigmoid',\\\n",
        "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
        "                                bias_regularizer=regularizers.l2(0.001),))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), optimizer='Nadam', metrics=[\"CategoricalAccuracy\"])"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 40, 64)            1280064   \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, 38, 128)           24704     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_12 (Glo (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 1,305,155\n",
            "Trainable params: 1,305,155\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6-JPRF1atSp",
        "outputId": "ce54bee6-dc09-4a4f-b647-8600769db7c0"
      },
      "source": [
        "epochs = 100\n",
        "history = model.fit(train_ds.shuffle(2000).batch(128),\n",
        "                    epochs= epochs ,\n",
        "                    validation_data=valid_ds.batch(128),\n",
        "                    verbose=1)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4870: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "189/189 [==============================] - 10s 50ms/step - loss: 1.0452 - categorical_accuracy: 0.5557 - val_loss: 0.8094 - val_categorical_accuracy: 0.6883\n",
            "Epoch 2/100\n",
            "189/189 [==============================] - 9s 45ms/step - loss: 0.7963 - categorical_accuracy: 0.7038 - val_loss: 0.7754 - val_categorical_accuracy: 0.7201\n",
            "Epoch 3/100\n",
            "189/189 [==============================] - 9s 46ms/step - loss: 0.7585 - categorical_accuracy: 0.7305 - val_loss: 0.7795 - val_categorical_accuracy: 0.7250\n",
            "Epoch 4/100\n",
            "189/189 [==============================] - 8s 45ms/step - loss: 0.7305 - categorical_accuracy: 0.7468 - val_loss: 0.7956 - val_categorical_accuracy: 0.7253\n",
            "Epoch 5/100\n",
            "189/189 [==============================] - 9s 45ms/step - loss: 0.7190 - categorical_accuracy: 0.7612 - val_loss: 0.8118 - val_categorical_accuracy: 0.7283\n",
            "Epoch 6/100\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.7015 - categorical_accuracy: 0.7813 - val_loss: 0.8251 - val_categorical_accuracy: 0.7268\n",
            "Epoch 7/100\n",
            "189/189 [==============================] - 9s 47ms/step - loss: 0.6853 - categorical_accuracy: 0.7984 - val_loss: 0.8524 - val_categorical_accuracy: 0.7250\n",
            "Epoch 8/100\n",
            "189/189 [==============================] - 9s 47ms/step - loss: 0.6738 - categorical_accuracy: 0.8112 - val_loss: 0.8903 - val_categorical_accuracy: 0.7156\n",
            "Epoch 9/100\n",
            "189/189 [==============================] - 9s 46ms/step - loss: 0.6537 - categorical_accuracy: 0.8300 - val_loss: 0.9117 - val_categorical_accuracy: 0.7126\n",
            "Epoch 10/100\n",
            "189/189 [==============================] - 9s 47ms/step - loss: 0.6461 - categorical_accuracy: 0.8378 - val_loss: 0.9170 - val_categorical_accuracy: 0.7194\n",
            "Epoch 11/100\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.6320 - categorical_accuracy: 0.8499 - val_loss: 0.9406 - val_categorical_accuracy: 0.7190\n",
            "Epoch 12/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.6205 - categorical_accuracy: 0.8593 - val_loss: 0.9546 - val_categorical_accuracy: 0.7149\n",
            "Epoch 13/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.6138 - categorical_accuracy: 0.8664 - val_loss: 0.9661 - val_categorical_accuracy: 0.7156\n",
            "Epoch 14/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.6038 - categorical_accuracy: 0.8709 - val_loss: 0.9777 - val_categorical_accuracy: 0.7119\n",
            "Epoch 15/100\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.5960 - categorical_accuracy: 0.8786 - val_loss: 1.0037 - val_categorical_accuracy: 0.7063\n",
            "Epoch 16/100\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.5898 - categorical_accuracy: 0.8832 - val_loss: 1.0090 - val_categorical_accuracy: 0.7022\n",
            "Epoch 17/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.5817 - categorical_accuracy: 0.8912 - val_loss: 1.0247 - val_categorical_accuracy: 0.7085\n",
            "Epoch 18/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.5737 - categorical_accuracy: 0.8935 - val_loss: 1.0409 - val_categorical_accuracy: 0.7085\n",
            "Epoch 19/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.5716 - categorical_accuracy: 0.8966 - val_loss: 1.0443 - val_categorical_accuracy: 0.7055\n",
            "Epoch 20/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.5647 - categorical_accuracy: 0.9019 - val_loss: 1.0559 - val_categorical_accuracy: 0.7067\n",
            "Epoch 21/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.5588 - categorical_accuracy: 0.9065 - val_loss: 1.0643 - val_categorical_accuracy: 0.7063\n",
            "Epoch 22/100\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.5539 - categorical_accuracy: 0.9083 - val_loss: 1.0841 - val_categorical_accuracy: 0.6906\n",
            "Epoch 23/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.5525 - categorical_accuracy: 0.9086 - val_loss: 1.0756 - val_categorical_accuracy: 0.7033\n",
            "Epoch 24/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.5506 - categorical_accuracy: 0.9105 - val_loss: 1.0905 - val_categorical_accuracy: 0.6951\n",
            "Epoch 25/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.5474 - categorical_accuracy: 0.9107 - val_loss: 1.0895 - val_categorical_accuracy: 0.6988\n",
            "Epoch 26/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.5419 - categorical_accuracy: 0.9153 - val_loss: 1.0983 - val_categorical_accuracy: 0.6999\n",
            "Epoch 27/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.5381 - categorical_accuracy: 0.9171 - val_loss: 1.0927 - val_categorical_accuracy: 0.6954\n",
            "Epoch 28/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.5352 - categorical_accuracy: 0.9203 - val_loss: 1.0993 - val_categorical_accuracy: 0.6951\n",
            "Epoch 29/100\n",
            "189/189 [==============================] - 8s 43ms/step - loss: 0.5354 - categorical_accuracy: 0.9191 - val_loss: 1.1096 - val_categorical_accuracy: 0.6928\n",
            "Epoch 30/100\n",
            "189/189 [==============================] - 8s 42ms/step - loss: 0.5305 - categorical_accuracy: 0.9215 - val_loss: 1.1202 - val_categorical_accuracy: 0.6928\n",
            "Epoch 31/100\n",
            "189/189 [==============================] - 9s 45ms/step - loss: 0.5303 - categorical_accuracy: 0.9219 - val_loss: 1.1123 - val_categorical_accuracy: 0.6932\n",
            "Epoch 32/100\n",
            "189/189 [==============================] - 9s 46ms/step - loss: 0.5274 - categorical_accuracy: 0.9239 - val_loss: 1.1122 - val_categorical_accuracy: 0.6973\n",
            "Epoch 33/100\n",
            "189/189 [==============================] - 9s 46ms/step - loss: 0.5227 - categorical_accuracy: 0.9264 - val_loss: 1.1205 - val_categorical_accuracy: 0.6947\n",
            "Epoch 34/100\n",
            "189/189 [==============================] - 9s 45ms/step - loss: 0.5194 - categorical_accuracy: 0.9262 - val_loss: 1.1195 - val_categorical_accuracy: 0.6928\n",
            "Epoch 35/100\n",
            "189/189 [==============================] - 8s 45ms/step - loss: 0.5204 - categorical_accuracy: 0.9263 - val_loss: 1.1204 - val_categorical_accuracy: 0.6951\n",
            "Epoch 36/100\n",
            "189/189 [==============================] - 8s 44ms/step - loss: 0.5175 - categorical_accuracy: 0.9287 - val_loss: 1.1214 - val_categorical_accuracy: 0.6932\n",
            "Epoch 37/100\n",
            "189/189 [==============================] - 8s 45ms/step - loss: 0.5127 - categorical_accuracy: 0.9307 - val_loss: 1.1324 - val_categorical_accuracy: 0.6939\n",
            "Epoch 38/100\n",
            "189/189 [==============================] - 9s 46ms/step - loss: 0.5135 - categorical_accuracy: 0.9311 - val_loss: 1.1330 - val_categorical_accuracy: 0.6891\n",
            "Epoch 39/100\n",
            "189/189 [==============================] - 9s 46ms/step - loss: 0.5080 - categorical_accuracy: 0.9332 - val_loss: 1.1315 - val_categorical_accuracy: 0.6969\n",
            "Epoch 40/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.5080 - categorical_accuracy: 0.9327 - val_loss: 1.1254 - val_categorical_accuracy: 0.6951\n",
            "Epoch 41/100\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.5064 - categorical_accuracy: 0.9333 - val_loss: 1.1398 - val_categorical_accuracy: 0.6906\n",
            "Epoch 42/100\n",
            "189/189 [==============================] - 9s 47ms/step - loss: 0.5018 - categorical_accuracy: 0.9353 - val_loss: 1.1399 - val_categorical_accuracy: 0.6865\n",
            "Epoch 43/100\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.5034 - categorical_accuracy: 0.9354 - val_loss: 1.1327 - val_categorical_accuracy: 0.6906\n",
            "Epoch 44/100\n",
            "189/189 [==============================] - 9s 47ms/step - loss: 0.5045 - categorical_accuracy: 0.9347 - val_loss: 1.1284 - val_categorical_accuracy: 0.6925\n",
            "Epoch 45/100\n",
            "189/189 [==============================] - 9s 47ms/step - loss: 0.4968 - categorical_accuracy: 0.9371 - val_loss: 1.1428 - val_categorical_accuracy: 0.6868\n",
            "Epoch 46/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.4976 - categorical_accuracy: 0.9364 - val_loss: 1.1465 - val_categorical_accuracy: 0.6816\n",
            "Epoch 47/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4946 - categorical_accuracy: 0.9382 - val_loss: 1.1448 - val_categorical_accuracy: 0.6887\n",
            "Epoch 48/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4938 - categorical_accuracy: 0.9403 - val_loss: 1.1477 - val_categorical_accuracy: 0.6876\n",
            "Epoch 49/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4945 - categorical_accuracy: 0.9386 - val_loss: 1.1463 - val_categorical_accuracy: 0.6865\n",
            "Epoch 50/100\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.4909 - categorical_accuracy: 0.9401 - val_loss: 1.1362 - val_categorical_accuracy: 0.6876\n",
            "Epoch 51/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4855 - categorical_accuracy: 0.9426 - val_loss: 1.1434 - val_categorical_accuracy: 0.6906\n",
            "Epoch 52/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4900 - categorical_accuracy: 0.9404 - val_loss: 1.1524 - val_categorical_accuracy: 0.6854\n",
            "Epoch 53/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.4872 - categorical_accuracy: 0.9421 - val_loss: 1.1531 - val_categorical_accuracy: 0.6846\n",
            "Epoch 54/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4862 - categorical_accuracy: 0.9426 - val_loss: 1.1549 - val_categorical_accuracy: 0.6820\n",
            "Epoch 55/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4862 - categorical_accuracy: 0.9425 - val_loss: 1.1563 - val_categorical_accuracy: 0.6831\n",
            "Epoch 56/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4862 - categorical_accuracy: 0.9428 - val_loss: 1.1568 - val_categorical_accuracy: 0.6839\n",
            "Epoch 57/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4872 - categorical_accuracy: 0.9419 - val_loss: 1.1598 - val_categorical_accuracy: 0.6820\n",
            "Epoch 58/100\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.4872 - categorical_accuracy: 0.9414 - val_loss: 1.1446 - val_categorical_accuracy: 0.6854\n",
            "Epoch 59/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4849 - categorical_accuracy: 0.9429 - val_loss: 1.1607 - val_categorical_accuracy: 0.6812\n",
            "Epoch 60/100\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.4801 - categorical_accuracy: 0.9453 - val_loss: 1.1575 - val_categorical_accuracy: 0.6831\n",
            "Epoch 61/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4808 - categorical_accuracy: 0.9453 - val_loss: 1.1524 - val_categorical_accuracy: 0.6891\n",
            "Epoch 62/100\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.4817 - categorical_accuracy: 0.9451 - val_loss: 1.1544 - val_categorical_accuracy: 0.6913\n",
            "Epoch 63/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4791 - categorical_accuracy: 0.9460 - val_loss: 1.1555 - val_categorical_accuracy: 0.6895\n",
            "Epoch 64/100\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.4756 - categorical_accuracy: 0.9465 - val_loss: 1.1553 - val_categorical_accuracy: 0.6824\n",
            "Epoch 65/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4735 - categorical_accuracy: 0.9480 - val_loss: 1.1587 - val_categorical_accuracy: 0.6831\n",
            "Epoch 66/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4735 - categorical_accuracy: 0.9463 - val_loss: 1.1611 - val_categorical_accuracy: 0.6831\n",
            "Epoch 67/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4762 - categorical_accuracy: 0.9446 - val_loss: 1.1476 - val_categorical_accuracy: 0.6898\n",
            "Epoch 68/100\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.4747 - categorical_accuracy: 0.9467 - val_loss: 1.1540 - val_categorical_accuracy: 0.6861\n",
            "Epoch 69/100\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.4746 - categorical_accuracy: 0.9476 - val_loss: 1.1515 - val_categorical_accuracy: 0.6883\n",
            "Epoch 70/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4758 - categorical_accuracy: 0.9474 - val_loss: 1.1602 - val_categorical_accuracy: 0.6928\n",
            "Epoch 71/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4766 - categorical_accuracy: 0.9466 - val_loss: 1.1581 - val_categorical_accuracy: 0.6857\n",
            "Epoch 72/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4755 - categorical_accuracy: 0.9464 - val_loss: 1.1597 - val_categorical_accuracy: 0.6880\n",
            "Epoch 73/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4741 - categorical_accuracy: 0.9480 - val_loss: 1.1481 - val_categorical_accuracy: 0.6880\n",
            "Epoch 74/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4694 - categorical_accuracy: 0.9517 - val_loss: 1.1516 - val_categorical_accuracy: 0.6883\n",
            "Epoch 75/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.4731 - categorical_accuracy: 0.9496 - val_loss: 1.1618 - val_categorical_accuracy: 0.6809\n",
            "Epoch 76/100\n",
            "189/189 [==============================] - 9s 48ms/step - loss: 0.4742 - categorical_accuracy: 0.9468 - val_loss: 1.1572 - val_categorical_accuracy: 0.6831\n",
            "Epoch 77/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.4757 - categorical_accuracy: 0.9460 - val_loss: 1.1510 - val_categorical_accuracy: 0.6891\n",
            "Epoch 78/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4724 - categorical_accuracy: 0.9487 - val_loss: 1.1594 - val_categorical_accuracy: 0.6902\n",
            "Epoch 79/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4707 - categorical_accuracy: 0.9512 - val_loss: 1.1581 - val_categorical_accuracy: 0.6842\n",
            "Epoch 80/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.4726 - categorical_accuracy: 0.9487 - val_loss: 1.1585 - val_categorical_accuracy: 0.6872\n",
            "Epoch 81/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.4714 - categorical_accuracy: 0.9487 - val_loss: 1.1589 - val_categorical_accuracy: 0.6883\n",
            "Epoch 82/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.4710 - categorical_accuracy: 0.9487 - val_loss: 1.1631 - val_categorical_accuracy: 0.6816\n",
            "Epoch 83/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.4697 - categorical_accuracy: 0.9494 - val_loss: 1.1495 - val_categorical_accuracy: 0.6891\n",
            "Epoch 84/100\n",
            "189/189 [==============================] - 9s 49ms/step - loss: 0.4666 - categorical_accuracy: 0.9517 - val_loss: 1.1554 - val_categorical_accuracy: 0.6842\n",
            "Epoch 85/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.4681 - categorical_accuracy: 0.9509 - val_loss: 1.1610 - val_categorical_accuracy: 0.6895\n",
            "Epoch 86/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.4669 - categorical_accuracy: 0.9512 - val_loss: 1.1678 - val_categorical_accuracy: 0.6801\n",
            "Epoch 87/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4669 - categorical_accuracy: 0.9513 - val_loss: 1.1555 - val_categorical_accuracy: 0.6857\n",
            "Epoch 88/100\n",
            "189/189 [==============================] - 9s 50ms/step - loss: 0.4671 - categorical_accuracy: 0.9505 - val_loss: 1.1492 - val_categorical_accuracy: 0.6883\n",
            "Epoch 89/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4638 - categorical_accuracy: 0.9509 - val_loss: 1.1555 - val_categorical_accuracy: 0.6861\n",
            "Epoch 90/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4667 - categorical_accuracy: 0.9508 - val_loss: 1.1591 - val_categorical_accuracy: 0.6835\n",
            "Epoch 91/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4633 - categorical_accuracy: 0.9514 - val_loss: 1.1542 - val_categorical_accuracy: 0.6891\n",
            "Epoch 92/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4653 - categorical_accuracy: 0.9509 - val_loss: 1.1595 - val_categorical_accuracy: 0.6753\n",
            "Epoch 93/100\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 0.4652 - categorical_accuracy: 0.9509 - val_loss: 1.1583 - val_categorical_accuracy: 0.6883\n",
            "Epoch 94/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4629 - categorical_accuracy: 0.9506 - val_loss: 1.1578 - val_categorical_accuracy: 0.6898\n",
            "Epoch 95/100\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 0.4615 - categorical_accuracy: 0.9526 - val_loss: 1.1602 - val_categorical_accuracy: 0.6790\n",
            "Epoch 96/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4632 - categorical_accuracy: 0.9529 - val_loss: 1.1634 - val_categorical_accuracy: 0.6790\n",
            "Epoch 97/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4636 - categorical_accuracy: 0.9517 - val_loss: 1.1662 - val_categorical_accuracy: 0.6741\n",
            "Epoch 98/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4649 - categorical_accuracy: 0.9509 - val_loss: 1.1580 - val_categorical_accuracy: 0.6906\n",
            "Epoch 99/100\n",
            "189/189 [==============================] - 10s 51ms/step - loss: 0.4602 - categorical_accuracy: 0.9539 - val_loss: 1.1533 - val_categorical_accuracy: 0.6850\n",
            "Epoch 100/100\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 0.4611 - categorical_accuracy: 0.9526 - val_loss: 1.1509 - val_categorical_accuracy: 0.6842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtbm7QJSbLbd",
        "outputId": "c83358b5-8c45-4f05-f2b7-e441dc5396c1"
      },
      "source": [
        "history.history"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'categorical_accuracy': [0.5556986331939697,\n",
              "  0.7038128972053528,\n",
              "  0.7305200099945068,\n",
              "  0.7467602491378784,\n",
              "  0.7611729502677917,\n",
              "  0.7812759876251221,\n",
              "  0.7983884215354919,\n",
              "  0.8111812472343445,\n",
              "  0.8299551606178284,\n",
              "  0.8378052711486816,\n",
              "  0.8498920202255249,\n",
              "  0.8592789769172668,\n",
              "  0.8664230108261108,\n",
              "  0.870908796787262,\n",
              "  0.8785927891731262,\n",
              "  0.8832447528839111,\n",
              "  0.8912194967269897,\n",
              "  0.8935039043426514,\n",
              "  0.8966190218925476,\n",
              "  0.9019355177879333,\n",
              "  0.9065043926239014,\n",
              "  0.9083319306373596,\n",
              "  0.9085811376571655,\n",
              "  0.9105333089828491,\n",
              "  0.9106994271278381,\n",
              "  0.9152683019638062,\n",
              "  0.9170958399772644,\n",
              "  0.9202525615692139,\n",
              "  0.9191311001777649,\n",
              "  0.9214985966682434,\n",
              "  0.9218723773956299,\n",
              "  0.9239076375961304,\n",
              "  0.9263997077941895,\n",
              "  0.926192045211792,\n",
              "  0.9263166785240173,\n",
              "  0.9287257194519043,\n",
              "  0.9306778311729431,\n",
              "  0.9310932159423828,\n",
              "  0.933169960975647,\n",
              "  0.9327130913734436,\n",
              "  0.9332530498504639,\n",
              "  0.9352882504463196,\n",
              "  0.9353713393211365,\n",
              "  0.9346652030944824,\n",
              "  0.9370742440223694,\n",
              "  0.9364097118377686,\n",
              "  0.9382372498512268,\n",
              "  0.9402724504470825,\n",
              "  0.9385695457458496,\n",
              "  0.9400647878646851,\n",
              "  0.9425984621047974,\n",
              "  0.9403555393218994,\n",
              "  0.9420999884605408,\n",
              "  0.9425569176673889,\n",
              "  0.9425153732299805,\n",
              "  0.9427645802497864,\n",
              "  0.9419338703155518,\n",
              "  0.941352367401123,\n",
              "  0.9428892135620117,\n",
              "  0.9453397393226624,\n",
              "  0.9452567100524902,\n",
              "  0.9451320767402649,\n",
              "  0.9459627866744995,\n",
              "  0.9465027451515198,\n",
              "  0.9480395317077637,\n",
              "  0.9462950825691223,\n",
              "  0.9446336627006531,\n",
              "  0.9466688632965088,\n",
              "  0.9475826621055603,\n",
              "  0.9473749995231628,\n",
              "  0.9466273188591003,\n",
              "  0.9463781118392944,\n",
              "  0.9479979872703552,\n",
              "  0.9517361521720886,\n",
              "  0.9495763182640076,\n",
              "  0.9467519521713257,\n",
              "  0.9460458755493164,\n",
              "  0.9486625790596008,\n",
              "  0.9511961936950684,\n",
              "  0.9487041234970093,\n",
              "  0.9487456679344177,\n",
              "  0.9486625790596008,\n",
              "  0.9494102001190186,\n",
              "  0.9516946077346802,\n",
              "  0.9508639574050903,\n",
              "  0.9511546492576599,\n",
              "  0.9513208270072937,\n",
              "  0.9505316615104675,\n",
              "  0.9509469866752625,\n",
              "  0.9508224129676819,\n",
              "  0.9513623714447021,\n",
              "  0.950905442237854,\n",
              "  0.9509469866752625,\n",
              "  0.9506146907806396,\n",
              "  0.9525668621063232,\n",
              "  0.9529407024383545,\n",
              "  0.9516946077346802,\n",
              "  0.950905442237854,\n",
              "  0.9539375305175781,\n",
              "  0.9526084065437317],\n",
              " 'loss': [1.0451945066452026,\n",
              "  0.796301543712616,\n",
              "  0.7585434317588806,\n",
              "  0.7305271625518799,\n",
              "  0.718971848487854,\n",
              "  0.701504647731781,\n",
              "  0.6852862238883972,\n",
              "  0.6737722158432007,\n",
              "  0.6537423729896545,\n",
              "  0.6461180448532104,\n",
              "  0.6319558620452881,\n",
              "  0.6205000281333923,\n",
              "  0.6138278245925903,\n",
              "  0.6037606000900269,\n",
              "  0.5959987640380859,\n",
              "  0.5898078680038452,\n",
              "  0.5816580653190613,\n",
              "  0.5736609697341919,\n",
              "  0.5716253519058228,\n",
              "  0.564700722694397,\n",
              "  0.5587819218635559,\n",
              "  0.5539181232452393,\n",
              "  0.5524833798408508,\n",
              "  0.5506497621536255,\n",
              "  0.5473576784133911,\n",
              "  0.5418763160705566,\n",
              "  0.5381260514259338,\n",
              "  0.5352237224578857,\n",
              "  0.535383403301239,\n",
              "  0.5305249691009521,\n",
              "  0.5303246378898621,\n",
              "  0.5273691415786743,\n",
              "  0.5226649045944214,\n",
              "  0.5194488167762756,\n",
              "  0.5204007029533386,\n",
              "  0.5175098180770874,\n",
              "  0.512687087059021,\n",
              "  0.5135270357131958,\n",
              "  0.5079955458641052,\n",
              "  0.508037269115448,\n",
              "  0.5063915252685547,\n",
              "  0.5017825365066528,\n",
              "  0.5034099817276001,\n",
              "  0.5044798851013184,\n",
              "  0.49681347608566284,\n",
              "  0.49763622879981995,\n",
              "  0.49461403489112854,\n",
              "  0.493843138217926,\n",
              "  0.4945066571235657,\n",
              "  0.49093255400657654,\n",
              "  0.485524445772171,\n",
              "  0.48998066782951355,\n",
              "  0.48716098070144653,\n",
              "  0.48624542355537415,\n",
              "  0.4861600697040558,\n",
              "  0.4862344563007355,\n",
              "  0.48722735047340393,\n",
              "  0.4872458279132843,\n",
              "  0.48494112491607666,\n",
              "  0.480142742395401,\n",
              "  0.4807894229888916,\n",
              "  0.4817146956920624,\n",
              "  0.479107141494751,\n",
              "  0.4755765199661255,\n",
              "  0.47353312373161316,\n",
              "  0.47354617714881897,\n",
              "  0.47620877623558044,\n",
              "  0.4746827185153961,\n",
              "  0.47464901208877563,\n",
              "  0.47582343220710754,\n",
              "  0.4766489863395691,\n",
              "  0.47551849484443665,\n",
              "  0.4741019010543823,\n",
              "  0.4693871736526489,\n",
              "  0.4731079041957855,\n",
              "  0.47424814105033875,\n",
              "  0.4757261574268341,\n",
              "  0.4724469482898712,\n",
              "  0.4707475006580353,\n",
              "  0.4726349413394928,\n",
              "  0.47142285108566284,\n",
              "  0.47095543146133423,\n",
              "  0.46965938806533813,\n",
              "  0.46661341190338135,\n",
              "  0.4681399464607239,\n",
              "  0.466861367225647,\n",
              "  0.4668513536453247,\n",
              "  0.4671137034893036,\n",
              "  0.46378210186958313,\n",
              "  0.46670880913734436,\n",
              "  0.4632938802242279,\n",
              "  0.4652605950832367,\n",
              "  0.46519261598587036,\n",
              "  0.4628540873527527,\n",
              "  0.4615047872066498,\n",
              "  0.4631865918636322,\n",
              "  0.4635826349258423,\n",
              "  0.4648744463920593,\n",
              "  0.4601871073246002,\n",
              "  0.4611097574234009],\n",
              " 'val_categorical_accuracy': [0.6883407831192017,\n",
              "  0.7201046347618103,\n",
              "  0.7249626517295837,\n",
              "  0.7253363132476807,\n",
              "  0.7283258438110352,\n",
              "  0.7268310785293579,\n",
              "  0.7249626517295837,\n",
              "  0.7156203389167786,\n",
              "  0.7126308083534241,\n",
              "  0.7193572521209717,\n",
              "  0.71898353099823,\n",
              "  0.7148729562759399,\n",
              "  0.7156203389167786,\n",
              "  0.7118834257125854,\n",
              "  0.7062780261039734,\n",
              "  0.7021673917770386,\n",
              "  0.7085201740264893,\n",
              "  0.7085201740264893,\n",
              "  0.7055306434631348,\n",
              "  0.7066517472267151,\n",
              "  0.7062780261039734,\n",
              "  0.6905829310417175,\n",
              "  0.7032884955406189,\n",
              "  0.695067286491394,\n",
              "  0.6988041996955872,\n",
              "  0.6999252438545227,\n",
              "  0.695440948009491,\n",
              "  0.695067286491394,\n",
              "  0.6928251385688782,\n",
              "  0.6928251385688782,\n",
              "  0.6931988000869751,\n",
              "  0.6973094344139099,\n",
              "  0.6946935653686523,\n",
              "  0.6928251385688782,\n",
              "  0.695067286491394,\n",
              "  0.6931988000869751,\n",
              "  0.6939461827278137,\n",
              "  0.6890881657600403,\n",
              "  0.6969357132911682,\n",
              "  0.695067286491394,\n",
              "  0.6905829310417175,\n",
              "  0.6864723563194275,\n",
              "  0.6905829310417175,\n",
              "  0.6924514174461365,\n",
              "  0.6868460178375244,\n",
              "  0.681614339351654,\n",
              "  0.6887145042419434,\n",
              "  0.687593400478363,\n",
              "  0.6864723563194275,\n",
              "  0.687593400478363,\n",
              "  0.6905829310417175,\n",
              "  0.6853512525558472,\n",
              "  0.6846038699150085,\n",
              "  0.6819880604743958,\n",
              "  0.6831091046333313,\n",
              "  0.6838564872741699,\n",
              "  0.6819880604743958,\n",
              "  0.6853512525558472,\n",
              "  0.6812406778335571,\n",
              "  0.6831091046333313,\n",
              "  0.6890881657600403,\n",
              "  0.6913303732872009,\n",
              "  0.689461886882782,\n",
              "  0.6823617219924927,\n",
              "  0.6831091046333313,\n",
              "  0.6831091046333313,\n",
              "  0.6898355484008789,\n",
              "  0.6860986351966858,\n",
              "  0.6883407831192017,\n",
              "  0.6928251385688782,\n",
              "  0.6857249736785889,\n",
              "  0.6879671216011047,\n",
              "  0.6879671216011047,\n",
              "  0.6883407831192017,\n",
              "  0.6808669567108154,\n",
              "  0.6831091046333313,\n",
              "  0.6890881657600403,\n",
              "  0.6902092695236206,\n",
              "  0.6842302083969116,\n",
              "  0.6872197389602661,\n",
              "  0.6883407831192017,\n",
              "  0.681614339351654,\n",
              "  0.6890881657600403,\n",
              "  0.6842302083969116,\n",
              "  0.689461886882782,\n",
              "  0.6801195740699768,\n",
              "  0.6857249736785889,\n",
              "  0.6883407831192017,\n",
              "  0.6860986351966858,\n",
              "  0.683482825756073,\n",
              "  0.6890881657600403,\n",
              "  0.6752615571022034,\n",
              "  0.6883407831192017,\n",
              "  0.6898355484008789,\n",
              "  0.6789985299110413,\n",
              "  0.6789985299110413,\n",
              "  0.6741405129432678,\n",
              "  0.6905829310417175,\n",
              "  0.6849775910377502,\n",
              "  0.6842302083969116],\n",
              " 'val_loss': [0.80943763256073,\n",
              "  0.7753806710243225,\n",
              "  0.7795212864875793,\n",
              "  0.7956341505050659,\n",
              "  0.8118316531181335,\n",
              "  0.8251175880432129,\n",
              "  0.8524025082588196,\n",
              "  0.8902567625045776,\n",
              "  0.9116981029510498,\n",
              "  0.9170179963111877,\n",
              "  0.9406338930130005,\n",
              "  0.9546139240264893,\n",
              "  0.9661445617675781,\n",
              "  0.9776591062545776,\n",
              "  1.00373113155365,\n",
              "  1.009006142616272,\n",
              "  1.024667501449585,\n",
              "  1.0409259796142578,\n",
              "  1.0443198680877686,\n",
              "  1.05585515499115,\n",
              "  1.0642532110214233,\n",
              "  1.0840747356414795,\n",
              "  1.0755544900894165,\n",
              "  1.0905044078826904,\n",
              "  1.0894715785980225,\n",
              "  1.0982733964920044,\n",
              "  1.092720627784729,\n",
              "  1.0992913246154785,\n",
              "  1.1095741987228394,\n",
              "  1.120224118232727,\n",
              "  1.1122760772705078,\n",
              "  1.1122322082519531,\n",
              "  1.12045419216156,\n",
              "  1.1195204257965088,\n",
              "  1.1204395294189453,\n",
              "  1.1214238405227661,\n",
              "  1.1323822736740112,\n",
              "  1.1330498456954956,\n",
              "  1.1314592361450195,\n",
              "  1.1254487037658691,\n",
              "  1.1398080587387085,\n",
              "  1.1398934125900269,\n",
              "  1.1326920986175537,\n",
              "  1.1284208297729492,\n",
              "  1.1428064107894897,\n",
              "  1.14652681350708,\n",
              "  1.1448122262954712,\n",
              "  1.1476737260818481,\n",
              "  1.1463197469711304,\n",
              "  1.136229157447815,\n",
              "  1.1434270143508911,\n",
              "  1.1523678302764893,\n",
              "  1.153051495552063,\n",
              "  1.154863953590393,\n",
              "  1.1563348770141602,\n",
              "  1.15677809715271,\n",
              "  1.1597542762756348,\n",
              "  1.1445928812026978,\n",
              "  1.1606703996658325,\n",
              "  1.157496452331543,\n",
              "  1.1523921489715576,\n",
              "  1.1543949842453003,\n",
              "  1.1555156707763672,\n",
              "  1.155305027961731,\n",
              "  1.158708095550537,\n",
              "  1.1610842943191528,\n",
              "  1.1475917100906372,\n",
              "  1.1539655923843384,\n",
              "  1.1515023708343506,\n",
              "  1.1601814031600952,\n",
              "  1.1581393480300903,\n",
              "  1.1596759557724,\n",
              "  1.1481070518493652,\n",
              "  1.1516166925430298,\n",
              "  1.1617931127548218,\n",
              "  1.157230257987976,\n",
              "  1.1509528160095215,\n",
              "  1.1593515872955322,\n",
              "  1.1580983400344849,\n",
              "  1.1585034132003784,\n",
              "  1.158888816833496,\n",
              "  1.1630908250808716,\n",
              "  1.1494920253753662,\n",
              "  1.1553966999053955,\n",
              "  1.1609904766082764,\n",
              "  1.167827844619751,\n",
              "  1.1554633378982544,\n",
              "  1.1492012739181519,\n",
              "  1.1555078029632568,\n",
              "  1.1590957641601562,\n",
              "  1.1541556119918823,\n",
              "  1.1595044136047363,\n",
              "  1.1582837104797363,\n",
              "  1.157769799232483,\n",
              "  1.1602160930633545,\n",
              "  1.1634197235107422,\n",
              "  1.1661769151687622,\n",
              "  1.1580357551574707,\n",
              "  1.1533026695251465,\n",
              "  1.1509181261062622]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqs_UiXggtb3",
        "outputId": "1e64d0c2-ddc2-4027-eb18-d609b4b1f913"
      },
      "source": [
        "model.save('tf_cnnmodel') \n",
        "json_string = tokenizer.to_json()\n",
        "\n",
        "phoneUrl1 = \"https://www.bajajfinservmarkets.in/emi-store/lg-w10-32-gb-smoky-gray-3-gb-ram-smartphone.html\"\n",
        "phoneUrl2 = \"https://www.bajajfinservmarkets.in/emi-store/oppo-a15s-64-gb-fancy-white-4-gb-ram-smartphone.html\"\n",
        "null = \"NULL\""
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: tf_cnnmodel/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqLRIOpAg8qS"
      },
      "source": [
        "import json\n",
        "with open('tokenizer.json', 'w') as outfile:\n",
        "    json.dump(json_string, outfile)"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3iu9XO3hI1r",
        "outputId": "a36eae37-5659-4122-c3e5-1a84ee3e11aa"
      },
      "source": [
        "new_model = tf.keras.models.load_model('tf_cnnmodel')\n",
        "new_model.summary()"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 40, 64)            1280064   \n",
            "_________________________________________________________________\n",
            "conv1d_12 (Conv1D)           (None, 38, 128)           24704     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_12 (Glo (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 1,305,155\n",
            "Trainable params: 1,305,155\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqfW4UJphNFb"
      },
      "source": [
        "with open('tokenizer.json') as json_file:\n",
        "    json_string = json.load(json_file)\n",
        "tokenizer1 = tf.keras.preprocessing.text.tokenizer_from_json(json_string)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HZxGH-XhWPo"
      },
      "source": [
        "labels = [phoneUrl1, phoneUrl2, null]\n",
        "\n",
        "message = input('Enter the sentence ')\n",
        "x_test  = tokenizer1.texts_to_sequences([message])\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=40)\n",
        "\n",
        "predictions = new_model.predict(x_test)\n",
        "predict_results = predictions.argmax(axis=1)\n",
        "print(labels[int(predict_results)])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}